# Example environment configuration for AI Model Tool Call Proxy Server
# Copy this file to .env and modify the values as needed

# Backend server configuration
BACKEND_HOST=localhost
BACKEND_PORT=8888
BACKEND_PROTOCOL=http

# Proxy server configuration  
PROXY_HOST=0.0.0.0
PROXY_PORT=5000

# Request settings
REQUEST_TIMEOUT=300

# Feature toggles
ENABLE_TOOL_CALL_CONVERSION=true

# Environment and logging
FLASK_ENV=development
LOG_LEVEL=INFO
DEBUG=true

# Example configurations for different backends:

# For LM Studio (default)
# BACKEND_HOST=localhost
# BACKEND_PORT=8888

# For Ollama
# BACKEND_HOST=localhost  
# BACKEND_PORT=11434

# For remote OpenAI API
# BACKEND_HOST=api.openai.com
# BACKEND_PORT=443
# BACKEND_PROTOCOL=https

# For production deployment
# FLASK_ENV=production
# LOG_LEVEL=WARNING
# DEBUG=false
# REQUEST_TIMEOUT=60